import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import metrics
#Allowing User to upload a dataset 
upfile = st.file_uploader("Upload your own CSV file here", type = "csv")
if upfile is not None: 
    st.write("Here is the uploaded file:", upfile.name)
    df = pd.read_csv(upfile)
    st.write("Data Uploaded:", df)
#Sample datasets 


####Supervised Machine Learning Models
###Defining X and y
sel_col = st.selectbo("select the columns you would like to use", options = df.columns)
if sel_col: 
    X = df.drop(columns = [sel_col])
    y = df[sel_col]

selectingfeat = st.multiselect("select the features you would like to use:", options = X.columns, default = list(X.columns))
X = X[selectingfeat]

st.write("Selected features for (X)", X.head())
st.write("the target", y.head())


#Splitting the data
split = st.slider('Input % for dividing dataset', 0, 100, 50)
split=split/100


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=split)
    
 
st.header("Machine Learning Selection")
mlselect= st.selectbox("",options=['Linear Regression', 'Logistic Regression', 'Decision Tree'])

##Decision Tree
if mlselect == 'Decision Tree':
    st.subheader('Select the maximum depth of Decision Tree')
maxdepth = st.slider('Maximum depth of decision tree', 1, 15, 5)

st.subheader('Hyperparameters')
critDT= st.selectbox("",options=['gini','entropy','log_loss'])

from sklearn.tree import DecisionTreeClassifier
dmodel = DecisionTreeClassifier(criterion = critDT, max_depth = maxdepth)
dmodel.fit(X_train, y_train)

#Evaluating the model
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
y_pred = dmodel.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
st.write(f"Accuracy: {accuracy:.2f}")

#making a confusion matrix 
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#
st.text("The Classification Report:")
st.text(classification_report(y_test, y_pred))

#visualizing the tree 
import graphviz
from sklearn import tree

# Generate and display the decision tree graph
dot_data = tree.export_graphviz(dmodel, 
                     feature_names = X_train.columns, #columns of features, will tell what making splits on
                     class_names = ["Not_Survived", "Survived"], #gives us our target values ##################### would need to change to make general 
                     filled = True) #tells us gini_index and values per class
st.graphviz_chart(dot_data)

#ROC and AUC 
from sklearn.metrics import roc_curve, roc_auc_score
# Get the predicted probabilities for the positive class (survival)
y_probs = dmodel.predict_proba(X_test)[:,1] #gives us the probability of our test and only of the the y value because [:,1]

# Calculate the False Positive Rate (FPR), True Positive Rate (TPR), and thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_probs) #will output the false positive rate and true positive rate
roc_auc = roc_auc_score(y_test, y_probs)
# Plotting the ROC curve
plt.figure(figsize=(8,6))
plt.plot(fpr,tpr, label = f'ROC Curve (AUC = {roc_auc})')
plt.plot([0,1], [0,1], linestyle = "--") #plots our random line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()
###################Is there anything i need to do so only for binary classifications??? 

##Logistic Regression
#creating the model
if mlselect == 'Logistic Regression':
    from sklearn.linear_model import LogisticRegression
    logrmodel = LogisticRegression()
    logrmodel.fit(X_train, y_train) 
    y_pred = logrmodel.predict(X_test) #predicting on test data, takes X_test and run through model to get set of predicted y values

# Calculating accuracy
accuracy = accuracy_score(y_test, y_pred) 
accuracy
# Confusion matrix for logistic regression
logrcm = confusion_matrix(y_test, y_pred)
logrcm
#matplotlib to turn creating a better looking confusion matrix
sns.heatmap(cm, annot = True, cmap = "Blues") #cmap to change the color scheme
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()
# Showing classification report
st.text("The Classification Report:")
st.text(classification_report(y_test, y_pred))

# Metrics
st.subheader("Error Metrics for Logistic Regression MOdel")
    
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')

st.write("### Classification Metrics")
st.write(f"Accuracy: {accuracy:.4f}")
st.write(f"Precision: {precision:.4f}")
st.write(f"Recall: {recall:.4f}")
st.write(f"F1 Score: {f1:.4f}")


##Linear Regression 
if mlselect== 'Linear Regression':
    from sklearn.linear_model import LinearRegression
    linregmodel = LinearRegression()
    linregmodel.fit(X_train, y_train)
    y_pred = linregmodel.predict(X_test) 